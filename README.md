## ğŸ§  Deep Feedforward Neural Network from Scratch

### ğŸ” Overview

This project implements a **Deep Feedforward Neural Network (DFNN)** from scratch using **NumPy**, without relying on high-level libraries like TensorFlow or PyTorch. It is designed for **binary classification** tasks on non-linearly separable datasets such as `make_moons`.

Key features:

* Manual implementation of **forward and backward propagation**
* Custom **activation functions**
* Use of **Binary Cross-Entropy Loss**
* Visualizations of the **training loss** and the **decision boundary**

---

### ğŸ“Œ Objectives

* Understand the fundamentals of neural networks
* Implement a DFNN manually (forward & backward pass)
* Visualize model performance
* Practice training on synthetic data

---

### ğŸ§° Technologies Used

* Python
* NumPy
* Matplotlib
* Scikit-learn (for data generation only)

---

### ğŸ§  Model Architecture

* Input: 2 neurons (2D features)
* Hidden Layers: 2 hidden layers (20 and 10 neurons)
* Output: 1 neuron (binary classification)
* Activations: ReLU for hidden layers, Sigmoid for output

---

### ğŸ“‰ Training Results

| Metric        | Value            |
| ------------- | ---------------- |
| Epochs        | 1000             |
| Learning Rate | 0.1              |
| Accuracy      | 84.50  |

---

### ğŸ“Š Visualizations

#### ğŸ“ˆ Loss Curve

![Loss Curve](assets/loss_curve.png)

#### ğŸ§­ Decision Boundary

![Decision Boundary](assets/decision_boundary.png)

---

### ğŸš€ How to Run

You can run this notebook in **Google Colab** or **locally in Jupyter**.

1. Clone the repository
2. Open `DeepFeedforwardNN.ipynb`
3. Run all cells
4. Modify `learning_rate`, `epochs`, or architecture to experiment

---

### ğŸ“ Project Structure

```
ğŸ“¦ DeepFeedforwardNN-FromScratch
â”œâ”€â”€ DeepFeedforwardNN.ipynb     # Full implementation
â”œâ”€â”€ README.md
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ loss_curve.png
â”‚   â””â”€â”€ decision_boundary.png
```

---

### ğŸ§  What I Learned

* How feedforward and backpropagation work internally
* Importance of activation functions and gradients
* Tuning hyperparameters like learning rate and epochs
* Visualizing decision boundaries for classification

---

### âœ… To-Do (Optional Extensions)

* [ ] Add train/test split and evaluate test accuracy
* [ ] Add support for multiple output neurons (multi-class)
* [ ] Implement Dropout or L2 Regularization
* [ ] Experiment with `make_circles` or XOR datasets
